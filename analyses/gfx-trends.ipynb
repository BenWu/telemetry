{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The entry-point to this analysis is at the very bottom of this file.\n",
    "# Look for the call to DoUpdate()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import ujson as json\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.plotly as py\n",
    "import operator\n",
    "import json, time, sys\n",
    "import datetime\n",
    "from __future__ import division\n",
    "from moztelemetry import get_pings, get_pings_properties, get_one_ping_per_client\n",
    "def fmt_date(d):\n",
    "    return d.strftime(\"%Y%m%d\")\n",
    "def jstime(d):\n",
    "    return time.mktime(d.timetuple())\n",
    "def repartition(pipeline):\n",
    "    return pipeline.repartition(MaxPartitions).cache()\n",
    "\n",
    "%pylab inline\n",
    "\n",
    "MaxPartitions = sc.defaultParallelism * 4\n",
    "\n",
    "# Keep this small (0.00001) for fast backfill testing.\n",
    "WeeklyFraction = 0.05\n",
    "\n",
    "# Amount of days Telemetry keeps.\n",
    "MaxHistoryInDays = datetime.timedelta(180)\n",
    "\n",
    "# Bucket we'll drop files into on S3. If this is None, we won't attempt any\n",
    "# S3 uploads, and the analysis will start from scratch.\n",
    "S3_BUCKET = 's3://telemetry-public-analysis-2/gfx-telemetry/data/'\n",
    "GITHUB_REPO = 'https://raw.githubusercontent.com/dvander/moz-gfx-telemetry'\n",
    "\n",
    "# List of jobs allowed to have a first-run (meaning no S3 content).\n",
    "BrandNewJobs = []\n",
    "\n",
    "# If true, backfill up to MaxHistoryInDays rather than the last update.\n",
    "ForceMaxBackfill = False\n",
    "\n",
    "# When executed as a cron job, the \"output\" folder already exists.\n",
    "# For consistency we  create it if it doesn't exist.\n",
    "!mkdir -p output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use this block to temporarily change parameters above.\n",
    "#ForceMaxBackfill = True\n",
    "#WeeklyFraction = 0.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ArchKey =               'environment/build/architecture'\n",
    "FxVersionKey =          'environment/build/version'\n",
    "Wow64Key =              'environment/system/isWow64'\n",
    "CpuKey =                'environment/system/cpu'\n",
    "GfxAdaptersKey =        'environment/system/gfx/adapters'\n",
    "GfxFeaturesKey =        'environment/system/gfx/features'\n",
    "OSNameKey =             'environment/system/os/name'\n",
    "OSVersionKey =          'environment/system/os/version'\n",
    "OSServicePackMajorKey = 'environment/system/os/servicePackMajor'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FirstValidDate = datetime.datetime.utcnow() - MaxHistoryInDays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Log spam eats up disk space, so we disable it.\n",
    "def quiet_logs(sc):\n",
    "  logger = sc._jvm.org.apache.log4j\n",
    "  logger.LogManager.getLogger(\"org\").setLevel(logger.Level.ERROR)\n",
    "  logger.LogManager.getLogger(\"akka\").setLevel(logger.Level.ERROR)\n",
    "quiet_logs(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is the entry-point to grabbing reduced, preformatted pings.\n",
    "def FetchAndFormat(start_date, end_date):\n",
    "    pings = GetRawPings(start_date, end_date)\n",
    "    pings = ReduceRawPings(pings)\n",
    "    pings = get_one_ping_per_client(pings)\n",
    "    pings = pings.map(Validate)\n",
    "    pings = pings.filter(lambda p: p.get('valid', False) == True)\n",
    "    return pings.cache()\n",
    "    \n",
    "def GetRawPings(start_date, end_date):\n",
    "    if isinstance(start_date, datetime.datetime):\n",
    "        start_date = fmt_date(start_date)\n",
    "    if isinstance(end_date, datetime.datetime):\n",
    "        end_date = fmt_date(end_date)\n",
    "    args = {\n",
    "        'fraction': WeeklyFraction,\n",
    "        'submission_date': (start_date, end_date),\n",
    "        'app': 'Firefox',\n",
    "        'schema': 'v4',\n",
    "    }\n",
    "    return get_pings(sc, **args)\n",
    "\n",
    "def ReduceRawPings(pings):\n",
    "    return get_pings_properties(pings, [\n",
    "        'clientId',\n",
    "        'creationDate',\n",
    "        ArchKey,\n",
    "        Wow64Key,\n",
    "        CpuKey,\n",
    "        FxVersionKey,\n",
    "        GfxAdaptersKey,\n",
    "        GfxFeaturesKey,\n",
    "        OSNameKey,\n",
    "        OSVersionKey,\n",
    "        OSServicePackMajorKey,\n",
    "    ])\n",
    "\n",
    "# Transform each ping to make it easier to work with in later stages.\n",
    "def Validate(p):\n",
    "    try:\n",
    "        name = p.get(OSNameKey) or 'w'\n",
    "        version = p.get(OSVersionKey) or '0'\n",
    "        if name == 'Linux':\n",
    "            p['OSVersion'] = None\n",
    "            p['OS'] = 'Linux'\n",
    "            p['OSName'] = 'Linux'\n",
    "        elif name == 'Windows_NT':\n",
    "            spmaj = p.get(OSServicePackMajorKey) or '0'\n",
    "            p['OSVersion'] = version + '.' + str(spmaj)\n",
    "            p['OS'] = 'Windows-' + version + '.' + str(spmaj)\n",
    "            p['OSName'] = 'Windows'\n",
    "        elif name == 'Darwin':\n",
    "            p['OSVersion'] = version\n",
    "            p['OS'] = 'Darwin-' + version\n",
    "            p['OSName'] = 'Darwin'\n",
    "        else:\n",
    "            p['OSVersion'] = version\n",
    "            p['OS'] = '{0}-{1}'.format(name, version)\n",
    "            p['OSName'] = name\n",
    "    except:\n",
    "        return p\n",
    "    \n",
    "    p['valid'] = True\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Profiler for debugging. Use in a |with| clause.\n",
    "class Prof(object):\n",
    "    level = 0\n",
    "    \n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "    def __enter__(self):\n",
    "        self.sout('Starting {0}... '.format(self.name))\n",
    "        self.start = datetime.datetime.now()\n",
    "        Prof.level += 1\n",
    "        return None\n",
    "    def __exit__(self, type, value, traceback):\n",
    "        Prof.level -= 1\n",
    "        self.end = datetime.datetime.now()\n",
    "        self.sout('... {0}: {1}s'.format(self.name, (self.end - self.start).total_seconds()))\n",
    "    def sout(self, s):\n",
    "        sys.stdout.write(('##' * Prof.level) + ' ')\n",
    "        sys.stdout.write(s)\n",
    "        sys.stdout.write('\\n')\n",
    "        sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helpers.\n",
    "def fix_vendor(vendorID):\n",
    "    if vendorID == u'Intel Open Source Technology Center':\n",
    "        return u'0x8086'\n",
    "    return vendorID\n",
    "\n",
    "def get_vendor(ping):\n",
    "    try:\n",
    "        adapter = ping[GfxAdaptersKey][0]\n",
    "        return fix_vendor(adapter['vendorID'])\n",
    "    except:\n",
    "        return 'unknown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A TrendBase encapsulates the data needed to visualize a trend.\n",
    "# It has four functions:\n",
    "#    prepare    (download from cache)\n",
    "#    willUpdate (check if update is needed)\n",
    "#    update     (add analysis data for a week of pings)\n",
    "#    finish     (upload back to cache)\n",
    "class TrendBase(object):\n",
    "    def __init__(self, name):\n",
    "        super(TrendBase, self).__init__()\n",
    "        self.name = name\n",
    "    \n",
    "    # Called before analysis starts.\n",
    "    def prepare(self):\n",
    "        print('Preparing {0}'.format(self.name))\n",
    "        return True\n",
    "    \n",
    "    # Called before querying pings for the week for the given date. Return\n",
    "    # false to indicate that this should no longer receive updates.\n",
    "    def willUpdate(self, date):\n",
    "        raise Exception('Return true or false')\n",
    "   \n",
    "    def update(self, pings, **kwargs):\n",
    "        raise Exception('NYI')\n",
    "        \n",
    "    def finish(self):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Given a list of trend objects, query weeks from the last sunday\n",
    "# and iterating backwards until no trend object requires an update.\n",
    "def DoUpdate(trends):\n",
    "    root = TrendGroup('root', trends)\n",
    "    root.prepare()\n",
    "        \n",
    "    # Start each analysis slice on a Sunday.\n",
    "    latest = MostRecentSunday()\n",
    "    end = latest\n",
    "    \n",
    "    while True:\n",
    "        start = end - datetime.timedelta(7)\n",
    "        assert latest.weekday() == 6\n",
    "        \n",
    "        if not root.willUpdate(start):\n",
    "            break\n",
    "        \n",
    "        try:\n",
    "            with Prof('fetch {0}'.format(start)) as _:\n",
    "                pings = FetchAndFormat(start, end)\n",
    "        except:\n",
    "            raise\n",
    "            \n",
    "        if not root.update(pings, start_date = start, end_date = end):\n",
    "            break\n",
    "            \n",
    "        end = start\n",
    "        \n",
    "    root.finish()\n",
    "    \n",
    "def MostRecentSunday():\n",
    "    now = datetime.datetime.utcnow()\n",
    "    this_morning = datetime.datetime(now.year, now.month, now.day)\n",
    "    if this_morning.weekday() == 6:\n",
    "        return this_morning\n",
    "    diff = datetime.timedelta(0 - this_morning.weekday() - 1)\n",
    "    return this_morning + diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A TrendGroup is a collection of TrendBase objects. It lets us\n",
    "# group similar trends together. For example, if five trends all\n",
    "# need to filter Windows pings, we can filter for Windows pings\n",
    "# once and cache the result, rather than redo the filter each\n",
    "# time.\n",
    "#\n",
    "# Trend groups keep an \"active\" list of trends that will probably\n",
    "# need another update. If any trend stops requesting data, it is\n",
    "# removed from the active list.\n",
    "class TrendGroup(TrendBase):\n",
    "    def __init__(self, name, trends):\n",
    "        super(TrendGroup, self).__init__(name)\n",
    "        self.trends = trends\n",
    "        self.active = []\n",
    "    \n",
    "    def prepare(self):\n",
    "        self.trends = [trend for trend in self.trends if trend.prepare()]\n",
    "        self.active = self.trends[:]\n",
    "        return len(self.trends) > 0\n",
    "            \n",
    "    def willUpdate(self, date):\n",
    "        self.active = [trend for trend in self.active if trend.willUpdate(date)]\n",
    "        return len(self.active) > 0\n",
    "    \n",
    "    def update(self, pings, **kwargs):\n",
    "        pings = pings.cache()\n",
    "        self.active = [trend for trend in self.active if trend.update(pings, **kwargs)]\n",
    "        return len(self.active) > 0\n",
    "            \n",
    "    def finish(self):\n",
    "        for trend in self.trends:\n",
    "            trend.finish()\n",
    "            \n",
    "# A Trend object takes a new set of pings for a week's worth of data,\n",
    "# analyzes it, and adds the result to the trend set. Trend sets are\n",
    "# cached in S3 as JSON.\n",
    "#\n",
    "# If the latest entry in the cache covers less than a full week of\n",
    "# data, the entry is removed so that week can be re-queried.\n",
    "class Trend(TrendBase):\n",
    "    def __init__(self, filename):\n",
    "        super(Trend, self).__init__(filename)\n",
    "        self.s3_path = os.path.join(S3_BUCKET, filename) if S3_BUCKET else None\n",
    "        self.local_path = os.path.join('output', filename)\n",
    "        self.cache = None\n",
    "        self.lastFullWeek = None\n",
    "        self.newDataPoints = []\n",
    "        \n",
    "    def query(self, pings):\n",
    "        raise Exception('NYI')\n",
    "        \n",
    "    def willUpdate(self, date):\n",
    "        if date < FirstValidDate:\n",
    "            return False\n",
    "        if self.lastFullWeek is not None and date <= self.lastFullWeek:\n",
    "            return False\n",
    "        return True\n",
    "    \n",
    "    def prepare(self):\n",
    "        self.cache = self.fetch_json()\n",
    "        if self.cache is None:\n",
    "            self.cache = {\n",
    "                'created': jstime(datetime.datetime.utcnow()),\n",
    "                'trend': [],\n",
    "            }\n",
    "        \n",
    "        # Make sure trends are sorted in ascending order.\n",
    "        self.cache['trend'] = self.cache['trend'] or []            \n",
    "        self.cache['trend'] = sorted(self.cache['trend'], key = lambda o: o['start'])\n",
    "        \n",
    "        if len(self.cache['trend']) and not ForceMaxBackfill:\n",
    "            lastDataPoint = self.cache['trend'][-1]\n",
    "            lastDataPointStart = datetime.datetime.utcfromtimestamp(lastDataPoint['start'])\n",
    "            lastDataPointEnd = datetime.datetime.utcfromtimestamp(lastDataPoint['end'])\n",
    "            print(lastDataPoint, lastDataPointStart, lastDataPointEnd)\n",
    "            if lastDataPointEnd - lastDataPointStart < datetime.timedelta(7):\n",
    "                # The last data point had less than a full week, so we stop at the\n",
    "                # previous week, and remove the incomplete datapoint.\n",
    "                self.lastFullWeek = lastDataPointStart - datetime.timedelta(7)\n",
    "                self.cache['trend'].pop()\n",
    "            else:\n",
    "                # The last data point covered a full week, so that's our stopping\n",
    "                # point.\n",
    "                self.lastFullWeek = lastDataPointStart\n",
    "                print(self.lastFullWeek)\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    # Optional hook - transform pings before querying.\n",
    "    def transformPings(self, pings):\n",
    "        return pings\n",
    "    \n",
    "    def update(self, pings, start_date, end_date, **kwargs):\n",
    "        with Prof('count {0}'.format(self.name)):\n",
    "            pings = self.transformPings(pings)\n",
    "            count = pings.count()\n",
    "        if count == 0:\n",
    "            print('WARNING: no pings in RDD')\n",
    "            return False\n",
    "        \n",
    "        with Prof('query {0}'.format(self.name)):\n",
    "            data = self.query(pings)\n",
    "        \n",
    "        self.newDataPoints.append({\n",
    "            'start': jstime(start_date),\n",
    "            'end': jstime(end_date),\n",
    "            'total': count,\n",
    "            'data': data,\n",
    "        })\n",
    "        return True\n",
    "            \n",
    "    def finish(self):\n",
    "        # If we're doing a maximum backfill, remove points from the cache that are\n",
    "        # after the least recent data point that we newly queried.\n",
    "        if ForceMaxBackfill and len(self.newDataPoints):\n",
    "            stopAt = self.newDataPoints[-1]['start']\n",
    "            lastIndex = None\n",
    "            for index, entry in enumerate(self.cache['trend']):\n",
    "                if entry['start'] >= stopAt:\n",
    "                    lastIndex = index\n",
    "                    break\n",
    "            if lastIndex is not None:\n",
    "                self.cache['trend'] = self.cache['trend'][:lastIndex]\n",
    "        \n",
    "        # Note: the backfill algorithm in DoUpdate() walks in reverse, so dates\n",
    "        # will be accumulated in descending order. The final list should be in\n",
    "        # ascending order, so we reverse.\n",
    "        self.cache['trend'] += self.newDataPoints[::-1]\n",
    "        \n",
    "        text = json.dumps(self.cache)\n",
    "        \n",
    "        with open(self.local_path, 'w') as fp:\n",
    "            fp.write(text)\n",
    "        \n",
    "        if self.s3_path:\n",
    "            try:\n",
    "                os.system(\"aws s3 cp {0} {1}\".format(self.local_path, self.s3_path))\n",
    "            except Exception as e:\n",
    "                print(\"Failed s3 upload: {0}\".format(e))\n",
    "            \n",
    "    def fetch_json(self):\n",
    "        if self.s3_path:\n",
    "            try:\n",
    "                os.system(\"aws s3 cp {0} {1}\".format(self.s3_path, self.local_path))\n",
    "                with open(self.local_path, 'r') as fp:\n",
    "                    return json.load(fp)\n",
    "            except:\n",
    "                if self.name not in BrandNewJobs:\n",
    "                    raise\n",
    "                return None\n",
    "        else:\n",
    "            try:\n",
    "                with open(self.local_path, 'r') as fp:\n",
    "                    return json.load(fp)\n",
    "            except:\n",
    "                pass\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FirefoxTrend(Trend):\n",
    "    def __init__(self):\n",
    "        super(FirefoxTrend, self).__init__('trend-firefox.json')\n",
    "        \n",
    "    def query(self, pings, **kwargs):\n",
    "        def get_version(p):\n",
    "            v = p.get(FxVersionKey, None)\n",
    "            if v is None or not isinstance(v, basestring):\n",
    "                return 'unknown'\n",
    "            return v.split('.')[0]\n",
    "        return pings.map(lambda p: (get_version(p),)).countByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class WindowsGroup(TrendGroup):\n",
    "    def __init__(self, trends):\n",
    "        super(WindowsGroup, self).__init__('Windows', trends)\n",
    "        \n",
    "    def update(self, pings, **kwargs):\n",
    "        pings = pings.filter(lambda p: p['OSName'] == 'Windows')\n",
    "        return super(WindowsGroup, self).update(pings, **kwargs)\n",
    "\n",
    "class WinverTrend(Trend):\n",
    "    def __init__(self):\n",
    "        super(WinverTrend, self).__init__('trend-windows-versions.json')\n",
    "        \n",
    "    def query(self, pings):\n",
    "        return pings.map(lambda p: (p['OSVersion'],)).countByKey()\n",
    "    \n",
    "class WinCompositorTrend(Trend):\n",
    "    def __init__(self):\n",
    "        super(WinCompositorTrend, self).__init__('trend-windows-compositors.json')\n",
    "        \n",
    "    def willUpdate(self, date):\n",
    "        # This metric didn't ship until Firefox 43.\n",
    "        if date < datetime.datetime(2015, 11, 15):\n",
    "            return False\n",
    "        return super(WinCompositorTrend, self).willUpdate(date)\n",
    "        \n",
    "    def query(self, pings):\n",
    "        return pings.map(lambda p: (self.get_compositor(p),)).countByKey()\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_compositor(p):\n",
    "        features = p.get(GfxFeaturesKey, None)\n",
    "        if features is None:\n",
    "            return 'none'\n",
    "        return features.get('compositor', 'none')\n",
    "    \n",
    "class WinArchTrend(Trend):\n",
    "    def __init__(self):\n",
    "        super(WinArchTrend, self).__init__('trend-windows-arch.json')\n",
    "        \n",
    "    def query(self, pings):\n",
    "        return pings.map(lambda p: (self.get_os_bits(p),)).countByKey()\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_os_bits(p):\n",
    "        arch = p.get(ArchKey, 'unknown')\n",
    "        if arch == 'x86-64':\n",
    "            return '64'\n",
    "        elif arch == 'x86':\n",
    "            if p.get(Wow64Key, False):\n",
    "                return '32_on_64'\n",
    "            return '32'\n",
    "        return 'unknown'\n",
    "\n",
    "# This group restricts pings to Windows Vista+, and must be inside a\n",
    "# group that restricts pings to Windows.\n",
    "class WindowsVistaPlusGroup(TrendGroup):\n",
    "    def __init__(self, trends):\n",
    "        super(WindowsVistaPlusGroup, self).__init__('Windows Vista+', trends)\n",
    "        \n",
    "    def update(self, pings, **kwargs):\n",
    "        pings = pings.filter(lambda p: not p['OSVersion'].startswith('5.1'))\n",
    "        return super(WindowsVistaPlusGroup, self).update(pings, **kwargs)\n",
    "\n",
    "class Direct2DTrend(Trend):\n",
    "    def __init__(self):\n",
    "        super(Direct2DTrend, self).__init__('trend-windows-d2d.json')\n",
    "    \n",
    "    def query(self, pings):\n",
    "        return pings.map(lambda p: (self.get_d2d(p),)).countByKey()\n",
    "    \n",
    "    def willUpdate(self, date):\n",
    "        # This metric didn't ship until Firefox 43.\n",
    "        if date < datetime.datetime(2015, 11, 15):\n",
    "            return False\n",
    "        return super(Direct2DTrend, self).willUpdate(date)\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_d2d(p):\n",
    "        try:\n",
    "            status = p[GfxFeaturesKey]['d2d']['status']\n",
    "            if status != 'available':\n",
    "                return status\n",
    "            return p[GfxFeaturesKey]['d2d']['version']\n",
    "        except:\n",
    "            return 'unknown'\n",
    "        \n",
    "class Direct3D11Trend(Trend):\n",
    "    def __init__(self):\n",
    "        super(Direct3D11Trend, self).__init__('trend-windows-d3d11.json')\n",
    "    \n",
    "    def query(self, pings):\n",
    "        return pings.map(lambda p: (self.get_d3d11(p),)).countByKey()\n",
    "    \n",
    "    def willUpdate(self, date):\n",
    "        # This metric didn't ship until Firefox 43.\n",
    "        if date < datetime.datetime(2015, 11, 15):\n",
    "            return False\n",
    "        return super(Direct3D11Trend, self).willUpdate(date)\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_d3d11(p):\n",
    "        try:\n",
    "            d3d11 = p[GfxFeaturesKey]['d3d11']\n",
    "            if d3d11['status'] != 'available':\n",
    "                return d3d11['status']\n",
    "            if d3d11.get('warp', False):\n",
    "                return 'warp'\n",
    "            return d3d11['version']\n",
    "        except:\n",
    "            return 'unknown'\n",
    "        \n",
    "class WindowsVendorTrend(Trend):\n",
    "    def __init__(self):\n",
    "        super(WindowsVendorTrend, self).__init__('trend-windows-vendors.json')\n",
    "        \n",
    "    def query(self, pings):\n",
    "        return pings.map(lambda p: (get_vendor(p),)).countByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Device generation trend - a little more complicated, since we download\n",
    "# the generation database to produce a mapping.\n",
    "class DeviceGenTrend(Trend):\n",
    "    deviceMap = None\n",
    "    \n",
    "    def __init__(self, vendor, vendorName):\n",
    "        super(DeviceGenTrend, self).__init__('trend-windows-device-gen-{0}.json'.format(vendorName))\n",
    "        self.vendorBlock = None\n",
    "        self.vendorID = vendor\n",
    "        \n",
    "    def prepare(self):\n",
    "        # Grab the vendor -> device -> gen map.\n",
    "        if not DeviceGenTrend.deviceMap:\n",
    "            import urllib2\n",
    "            obj = urllib2.urlopen('{0}/master/www/gfxdevices.json'.format(GITHUB_REPO))\n",
    "            text = obj.read()\n",
    "            DeviceGenTrend.deviceMap = json.loads(text)\n",
    "        self.vendorBlock = DeviceGenTrend.deviceMap[self.vendorID]\n",
    "        return super(DeviceGenTrend, self).prepare()\n",
    "    \n",
    "    def transformPings(self, pings):\n",
    "        return pings.filter(lambda p: get_vendor(p) == self.vendorID)\n",
    "        \n",
    "    def query(self, pings):\n",
    "        return pings.map(lambda p: (self.get_gen(p),)).countByKey()\n",
    "    \n",
    "    def get_gen(self, p):\n",
    "        adapter = p[GfxAdaptersKey][0]\n",
    "        deviceID = adapter.get('deviceID', 'unknown')\n",
    "        if deviceID not in self.vendorBlock:\n",
    "            return 'unknown'\n",
    "        return self.vendorBlock[deviceID][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DoUpdate([\n",
    "    FirefoxTrend(),\n",
    "    WindowsGroup([\n",
    "        WinverTrend(),\n",
    "        WinCompositorTrend(),\n",
    "        WinArchTrend(),\n",
    "        WindowsVendorTrend(),\n",
    "        WindowsVistaPlusGroup([\n",
    "            Direct2DTrend(),\n",
    "            Direct3D11Trend(),\n",
    "        ]),\n",
    "        DeviceGenTrend(u'0x8086', 'intel'),\n",
    "        DeviceGenTrend(u'0x10de', 'nvidia'),\n",
    "        DeviceGenTrend(u'0x1002', 'amd'),\n",
    "    ])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
