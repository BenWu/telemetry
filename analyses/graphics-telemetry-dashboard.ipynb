{"nbformat_minor": 0, "cells": [{"execution_count": null, "cell_type": "code", "source": "import ujson as json\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport plotly.plotly as py\nimport operator\nimport json, time\nfrom __future__ import division\nfrom moztelemetry import get_pings, get_pings_properties, get_one_ping_per_client\ndef fmt_date(d):\n    return d.strftime(\"%Y%m%d\")\ndef repartition(pipeline):\n    return pipeline.repartition(MaxPartitions).cache()\n\n%pylab inline\n\nMaxPartitions = sc.defaultParallelism * 4\nStartTime = datetime.datetime.now()\n\n# Change this to False for local use.\nRUN_AS_TELEMETRY_JOB = False\n\n# Configuration for general data that spans all Firefox versions.\nGeneralTimeWindow = 14\nGeneralPingFraction = 0.40\nSanityTestPingFraction = 0.5", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": null, "cell_type": "code", "source": "###########################################################\n# Helper function block for fetching and filtering pings. #\n###########################################################\n\ndef union_pipelines(a, b):\n    if a is None:\n        return b\n    return a + b\n\ndef FetchRawPings(timeWindow, fraction, **kwargs):\n    channel = kwargs.pop('channel', None)\n    availDate = kwargs.pop('availDate', None)\n    \n    t1 = datetime.datetime.now() - datetime.timedelta(timeWindow)\n    if availDate is not None:\n        if availDate > t1:\n            t1 = availDate\n            availDays = (datetime.datetime.now() - t1).days\n            assert availDays >= 0\n\n            # Scale the fraction up to get roughly the same amount of pings.\n            # Derivation, f=fraction, z=pings per day, d=days, a=available days\n            #\n            #     x*z*a = f*z*d\n            #     x*a = f*d\n            #     x = (f*d)/a\n            fraction = min((fraction * timeWindow) / availDays, 1)\n            timeWindow = availDays\n            \n    t1 = fmt_date(t1)\n    t2 = fmt_date(datetime.datetime.now() - datetime.timedelta(0, 60*60)) # go back 1 hour\n    \n    kwargs = {\n        'app': 'Firefox',\n        'build_id': (t1, t2),\n        'fraction': fraction,\n    }\n    info = {\n        'fraction': fraction,\n        'timeWindow': timeWindow,\n        'channel': channel,\n        'timestamp': datetime.datetime.utcnow(),\n    }\n    if isinstance(channel, tuple) or isinstance(channel, list):\n        pings = None\n        for c in channel:\n            kwargs['channel'] = c\n            pings = union_pipelines(pings, get_pings(sc, **kwargs))\n    else:\n        pings = get_pings(sc, **kwargs)\n    return pings, info\n\n# Transform each ping to make it easier to work with in later stages.\ndef Validate(p):\n    name = p.get(\"environment/system/os/name\") or 'w'\n    version = p.get(\"environment/system/os/version\") or '0'\n    if name == 'Linux':\n        p['OSVersion'] = None\n        p['OS'] = 'Linux'\n        p['OSName'] = 'Linux'\n    elif name == 'Windows_NT':\n        spmaj = p.get(\"environment/system/os/servicePackMajor\") or '0'\n        p['OSVersion'] = version + '.' + str(spmaj)\n        p['OS'] = 'Windows-' + version + '.' + str(spmaj)\n        p['OSName'] = 'Windows'\n    elif name == 'Darwin':\n        p['OSVersion'] = version\n        p['OS'] = 'Darwin-' + version\n        p['OSName'] = 'Darwin'\n    else:\n        return p\n    \n    # Telemetry data isn't guaranteed to be well-formed so unfortunately\n    # we have to do some validation on it. If we get to the end, we set\n    # p['valid'] to True, and this gets filtered over later. In addition\n    # we have a wrapper below to help fetch strings that may be null.\n    if not p.get(\"environment/build/version\", None):\n        return p\n    p['FxVersion'] = p[\"environment/build/version\"].split('.')[0]\n    \n    # Verify that we have at least one adapter.\n    try:\n        adapter = p[\"environment/system/gfx/adapters\"][0]\n    except:\n        return p\n    if adapter is None or not hasattr(adapter, '__getitem__'):\n        return p\n    \n    def T(obj, key):\n        return obj.get(key, None) or 'Unknown'\n    \n    # We store the device ID as a vendor/device string, because the device ID\n    # alone is not enough to determine whether the key is unique.\n    #\n    # We also merge 'Intel Open Source Technology Center' with the device ID\n    # that should be reported, 0x8086, for simplicity.\n    vendorID = T(adapter, 'vendorID')\n    if vendorID == u'Intel Open Source Technology Center':\n        p['vendorID'] = u'0x8086'\n    else:\n        p['vendorID'] = vendorID\n    p['deviceID'] = u'{0}/{1}'.format(p['vendorID'], T(adapter, 'deviceID'))\n    p['driverVersion'] = u'{0}/{1}'.format(p['vendorID'], T(adapter, 'driverVersion'))\n    if adapter['driverVersion']:\n        p['driverShortVersion'] = '{0}/{1}'.format(p['vendorID'], '.'.join(T(adapter, 'driverVersion').split('.')[0:3]))\n    else:\n        p['driverShortVersion'] = '{0}/Unknown'.format(p['vendorID'])\n        \n    p['userPrefs'] = None\n    if p.get('environment/settings', None) is not None:\n        if p['environment/settings'].get('userPrefs', None) is not None:\n            p['userPrefs'] = p['environment/settings']['userPrefs']\n        \n    p['valid'] = True\n    return p\n\ndef reduce_pings(pings, schema='v2'):\n    if schema == 'v4':\n        prefix = 'payload/'\n        clientId = 'clientId'\n    else:\n        prefix = ''\n        clientId = 'clientID'\n    return get_pings_properties(pings, [\n      clientId,\n      \"creationDate\",\n      \"environment/settings\",\n      \"environment/build/version\",\n      \"environment/build/buildId\",\n      \"environment/system/os/name\",\n      \"environment/system/os/version\",\n      \"environment/system/os/servicePackMajor\",\n      \"environment/system/os/servicePackMinor\",\n      \"environment/system/gfx/adapters\",\n      \"environment/system/gfx/features\",\n      \"environment/system/gfx/monitors\",\n      prefix + \"histograms/DEVICE_RESET_REASON\",\n      prefix + \"histograms/GRAPHICS_SANITY_TEST\",\n      prefix + \"histograms/GRAPHICS_SANITY_TEST_REASON\",\n      prefix + \"histograms/GRAPHICS_DRIVER_STARTUP_TEST\",\n      prefix + \"info/revision\",\n    ])\n\ndef FormatPings(pings, schema='v2'):\n    pings = reduce_pings(pings, schema)\n    pings = get_one_ping_per_client(pings)\n    pings = pings.map(Validate)\n    filtered_pings = pings.filter(lambda p: p.get('valid', False) == True)\n    return filtered_pings.cache()\n\ndef FetchAndFormat(timeWindow, fraction, **kwargs):\n    raw_pings, info = FetchRawPings(timeWindow, fraction, **kwargs)\n    return FormatPings(raw_pings, kwargs.pop('schema', 'v2')), info", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": null, "cell_type": "code", "source": "##################################################################\n# Helper function block for massaging pings into aggregate data. #\n##################################################################\n\n# Take each key in |b| and add it to |a|, accumulating its value into\n# |a| if it already exists.\ndef combiner(a, b):\n    result = a\n    for key in b:\n        countA = a.get(key, 0)\n        countB = b[key]\n        result[key] = countA + countB\n    return result\n\n# Return an aggregation based on combiner.\ndef aggregation(data, fn):\n    view = data.map(fn)\n    return view.reduceByKey(combiner)\n\n# Helper for reduceByKey.\ndef map_x_to_y(data, sourceKey, destKey):\n    def extract(p):\n        return (p[sourceKey], { p[destKey]: 1 })\n    return aggregation(data, extract)\n\n# Helper for reduceByKey => count.\ndef map_x_to_count(data, sourceKey):\n    def extract(p):\n        return (p[sourceKey],)\n    return data.map(extract).countByKey()\n\n# After reduceByKey(combiner), we get a mapping like:\n#  key => { variable => value }\n#\n# This function collapses 'variable' instances below a threshold into\n# a catch-all identifier ('Other').\ndef coalesce_to_n_items(agg, max_items):\n    obj = []\n    for superkey, breakdown in agg:\n        if len(breakdown) <= max_items:\n            obj += [(superkey, breakdown)]\n            continue\n        items = sorted(breakdown.items(), key=lambda obj: obj[1], reverse=True)\n        new_breakdown = {k: v for k, v in items[0:max_items]}\n        total = 0\n        for k, v in items[max_items:]:\n            total += v\n        if total:\n            new_breakdown['Other'] = new_breakdown.get('Other', 0) + total\n        obj += [(superkey, new_breakdown)]\n    return obj", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": null, "cell_type": "code", "source": "#############################\n# Helper for writing files. #\n#############################\n\ndef ApplyPingInfo(obj, **kwargs):\n    if 'pings' not in kwargs:\n        return\n    pings, info = kwargs.pop('pings')\n    obj['sessions'] = {\n        'count': pings.count(),\n        'days': info['timeWindow'],\n        'timestamp': time.mktime(info['timestamp'].timetuple()),\n        'fraction': info['fraction'],\n        'channels': info['channel'],\n    }\n    \ndef Export(filename, obj):\n    if RUN_AS_TELEMETRY_JOB:\n        filename = os.path.join('output', filename)\n    with open(filename, 'w') as fp:\n        json.dump(obj, fp)\n        \ndef TimedExport(filename, callback, **kwargs):\n    start = datetime.datetime.now()\n    \n    obj = callback()\n    ApplyPingInfo(obj, **kwargs)\n    \n    end = datetime.datetime.now()\n    elapsed = end - start\n    obj['phaseTime'] = elapsed.total_seconds()\n    \n    Export(filename, obj)\n    export_time = datetime.datetime.now() - end\n    \n    print('Computed {0} in {1} seconds.'.format(filename, elapsed.total_seconds()))\n    print('Exported {0} in {1} seconds.'.format(filename, export_time.total_seconds()))", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": null, "cell_type": "code", "source": "# Get a general ping sample across all Firefox channels.\nGeneralPings, GeneralPingInfo = FetchAndFormat(GeneralTimeWindow, GeneralPingFraction)\n\n# Windows gets some preferential breakdown treatment.\nWindowsPings = GeneralPings.filter(lambda p: p['OSName'] == 'Windows')\nWindowsPings = WindowsPings.cache()", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": null, "cell_type": "code", "source": "# Results by operating system.\ndef GetGeneralStatistics():\n    # OSToVendor = map_x_to_y(GeneralPings, 'OSName', 'vendorID')\n    # OSToDevice = map_x_to_y(GeneralPings, 'OSName', 'deviceID')\n    OSShare = map_x_to_count(GeneralPings, 'OSName')\n\n    # Results by Windows version.\n    # WindowsToVendor = map_x_to_y(WindowsPings, 'OSVersion', 'vendorID')\n    # WindowsToDevice = map_x_to_y(WindowsPings, 'OSVersion', 'deviceID')\n    # WindowsToDriver = map_x_to_y(WindowsPings, 'OSVersion', 'driverVersion')\n    WindowsShare = map_x_to_count(WindowsPings, 'OSVersion')\n    DriverShare = map_x_to_count(WindowsPings, 'driverVersion')\n\n    # Results by Firefox version.\n    # FxToVendor = map_x_to_y(AllData, 'FxVersion', 'vendorID')\n    # FxToDevice = map_x_to_y(AllData, 'FxVersion', 'deviceID')\n    FxShare = map_x_to_count(GeneralPings, 'FxVersion')\n\n    # Top-level stats.\n    VendorShare = map_x_to_count(GeneralPings, 'vendorID')\n    DeviceShare = map_x_to_count(GeneralPings, 'deviceID')\n    \n    return {\n        'os': OSShare,\n        'windows': WindowsShare,\n        'firefox': FxShare,\n        'vendors': VendorShare,\n        'devices': DeviceShare,\n    }\n        \nTimedExport(filename = 'general-statistics.json',\n            callback = GetGeneralStatistics,\n            pings = (GeneralPings, GeneralPingInfo))", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": null, "cell_type": "code", "source": "#############################\n# Perform the TDR analysis. #\n#############################\ndef GetTDRStatistics():\n    NumTDRReasons = 8\n    def ping_has_tdr_for(p, reason):\n        return p['histograms/DEVICE_RESET_REASON'][reason] > 0\n\n    # Specialized version of map_x_to_y, for TDRs. We cast to int because for\n    # some reason the values Spark returns do not serialize with JSON.\n    def map_reason_to_vendor(p, reason, destKey):\n        return (int(reason), { p[destKey]: int(p['histograms/DEVICE_RESET_REASON'][reason]) })\n    def map_vendor_to_reason(p, reason, destKey):\n        return (p[destKey], { int(reason): int(p['histograms/DEVICE_RESET_REASON'][reason]) })\n\n    # Filter out pings that do not have any TDR data. We expect this to be a huge reduction\n    # in the sample set, and the resulting partition count gets way off. We repartition\n    # immediately for performance.\n    TDRSubset = WindowsPings.filter(lambda p: p.get('histograms/DEVICE_RESET_REASON', None) is not None)\n    TDRSubset = TDRSubset.repartition(MaxPartitions)\n    TDRSubset = TDRSubset.cache()\n\n    # Aggregate the device reset data.\n    TDRResults = TDRSubset.map(lambda p: p['histograms/DEVICE_RESET_REASON']).reduce(lambda x, y: x + y)\n\n    # For each TDR reason, get a list tuple of (reason, vendor => resetCount). Then\n    # we combine these into a single series.\n    reason_to_vendor_tuples = None\n    vendor_to_reason_tuples = None\n    for reason in xrange(1, NumTDRReasons):\n        subset = TDRSubset.filter(lambda p: ping_has_tdr_for(p, reason))\n        subset = subset.cache()\n\n        tuples = subset.map(lambda p: map_reason_to_vendor(p, reason, 'vendorID'))\n        reason_to_vendor_tuples = union_pipelines(reason_to_vendor_tuples, tuples)\n\n        tuples = subset.map(lambda p: map_vendor_to_reason(p, reason, 'vendorID'))\n        vendor_to_reason_tuples = union_pipelines(vendor_to_reason_tuples, tuples)\n\n    TDRReasonToVendor = reason_to_vendor_tuples.reduceByKey(combiner, MaxPartitions)\n    TDRVendorToReason = vendor_to_reason_tuples.reduceByKey(combiner, MaxPartitions)\n    \n    return {\n        'tdrPings': TDRSubset.count(),\n        'results': [int(value) for value in TDRResults],\n        'reasonToVendor': TDRReasonToVendor.collect(),\n        'vendorToReason': TDRVendorToReason.collect(),\n    }\n    \n# Write TDR statistics.\nTimedExport(filename = 'tdr-statistics.json',\n            callback = GetTDRStatistics,\n            pings = (WindowsPings, GeneralPingInfo))", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": null, "cell_type": "code", "source": "# Get pings with graphics features. This landed in roughly the 7-19-2015 nightly.\nFeaturesKey = 'environment/system/gfx/features'\n\nWindowsFeaturePings, WindowsFeaturePingsInfo = FetchAndFormat(\n    timeWindow = GeneralTimeWindow,\n    fraction = SanityTestPingFraction,\n    channel = 'nightly',\n    availDate = datetime.datetime(2015, 7, 19, 18, 20, 59, 378130))\n\ndef windows_feature_filter(p):\n    return p['OSName'] == 'Windows' and p.get(FeaturesKey) is not None\n\nWindowsFeatures = WindowsFeaturePings.filter(windows_feature_filter)\nWindowsFeatures = WindowsFeatures.cache()", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": null, "cell_type": "code", "source": "# Build graphics feature statistics.\ndef get_compositor(p):\n    compositor = p[FeaturesKey].get('compositor', 'none')\n    if compositor == 'none':\n        userPrefs = p['userPrefs']\n        if userPrefs is not None:\n            omtc = userPrefs.get('layers.offmainthreadcomposition.enabled', True)\n            if omtc != True:\n                compositor = 'disabled'\n    return (compositor,)\n\ndef get_d3d11_status(p):\n    d3d11 = p[FeaturesKey].get('d3d11', None)\n    if not hasattr(d3d11, '__getitem__'):\n        return 'unknown'\n    status = d3d11.get('status', 'unknown')\n    if status != 'available':\n        return status\n    if d3d11.get('warp', False) == True:\n        return 'warp'\n    return d3d11.get('version', 'unknown')\n\ndef get_warp_status(p):\n    if 'blacklisted' not in p[FeaturesKey]['d3d11']:\n        return 'unknown'\n    if p[FeaturesKey]['d3d11']['blacklisted'] == True:\n        return 'blacklist'\n    return 'device failure'\n\ndef get_d2d_status(p):\n    d2d = p[FeaturesKey].get('d2d', None)\n    if not hasattr(d2d, '__getitem__'):\n        return ('unknown',)\n    status = d2d.get('status', 'unknown')\n    if status != 'available':\n        return (status,)\n    return (d2d.get('version', 'unknown'),)\n\ndef has_working_d3d11(p):\n    d3d11 = p[FeaturesKey].get('d3d11', None)\n    if d3d11 is None:\n        return False\n    return d3d11.get('status') == 'available'\n\ndef get_texture_sharing_status(p):\n    return (p[FeaturesKey]['d3d11'].get('textureSharing', 'unknown'),)\n\n# We skip certain windows versions in detail lists since this phase is\n# very expensive to compute.\nImportantWindowsVersions = (\n    '5.1.2',\n    '5.1.3',\n    '6.0.2',\n    '6.1.0',\n    '6.1.1',\n    '6.2.0',\n    '6.3.0',\n    '10.0.0',\n)\n\ndef GetWindowsFeatures():\n    WindowsCompositorMap = WindowsFeatures.map(get_compositor).countByKey()\n    D3D11StatusMap = WindowsFeatures.map(lambda p: (get_d3d11_status(p),)).countByKey()\n    D2DStatusMap = WindowsFeatures.map(get_d2d_status).countByKey()\n    \n    warp_pings = WindowsFeatures.filter(lambda p: get_d3d11_status(p) == 'warp')\n    warp_pings = repartition(warp_pings)\n    WarpStatusMap = warp_pings.map(lambda p: (get_warp_status(p),)).countByKey()\n\n    TextureSharingMap = WindowsFeatures.filter(has_working_d3d11).map(get_texture_sharing_status).countByKey()\n\n    # Now, build the same data except per version.\n    feature_pings_by_os = map_x_to_count(WindowsFeatures, 'OSVersion')\n    WindowsFeaturesByVersion = {}\n    for os_version in feature_pings_by_os:\n        if os_version not in ImportantWindowsVersions:\n            continue\n        subset = WindowsFeatures.filter(lambda p: p['OSVersion'] == os_version)\n        subset = repartition(subset)\n        \n        results = {\n            'count': subset.count(),\n            'compositors': subset.map(get_compositor).countByKey(),\n        }\n        try:\n            if int(os_version.split('.')[0]) >= 6:\n                results['d3d11'] = subset.map(lambda p: (get_d3d11_status(p),)).countByKey()\n                results['d2d'] = subset.map(get_d2d_status).countByKey()\n                \n                warp_pings = subset.filter(lambda p: get_d3d11_status(p) == 'warp')\n                results['warp'] = warp_pings.map(lambda p: (get_warp_status(p),)).countByKey()\n        except:\n            pass\n        finally:\n            # Free resources.\n            warp_pings = None\n            subset = None\n        WindowsFeaturesByVersion[os_version] = results\n    \n    return {\n        'all': {\n            'compositors': WindowsCompositorMap,\n            'd3d11': D3D11StatusMap,\n            'd2d': D2DStatusMap,\n            'textureSharing': TextureSharingMap,\n            'warp': WarpStatusMap,\n        },\n        'byVersion': WindowsFeaturesByVersion,\n    }\n\nTimedExport(filename = 'windows-features.json',\n            callback = GetWindowsFeatures,\n            pings = (WindowsFeatures, WindowsFeaturePingsInfo))", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": null, "cell_type": "code", "source": "# Sanity test and crash analysis setup.\n# The sanity test was only available in Firefox 41 and higher, so currently\n# our analyses grabs a different sample of pings to be more accurate.\n#\n# Once the sanity test makes it to beta we can use the main data set.\nsanity_test_ping_pool, sanity_test_info = FetchAndFormat(\n    timeWindow = GeneralTimeWindow,\n    fraction = SanityTestPingFraction,\n    channel = ('nightly', 'aurora'))\n\n# Set up constants.\nSANITY_TEST_PASSED = 0\nSANITY_TEST_FAILED_RENDER = 1\nSANITY_TEST_FAILED_VIDEO = 2\nSANITY_TEST_CRASHED = 3\nSANITY_TEST_LAST_VALUE = 4\nSANITY_TEST_REASON_FIRST_RUN = 0\nSANITY_TEST_REASON_FIREFOX_CHANGED = 1\nSANITY_TEST_REASON_DEVICE_CHANGED = 2\nSANITY_TEST_REASON_DRIVRE_CHANGED = 3\nSAINTY_TEST_REASON_LAST_VALUE = 4\n\nSANITY_TEST = 'histograms/GRAPHICS_SANITY_TEST'\nSANITY_TEST_REASON = 'histograms/GRAPHICS_SANITY_TEST_REASON'", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": null, "cell_type": "code", "source": "# Filter sanity test pings.\ndef ping_has_sanity_test(p):\n    return p.get(SANITY_TEST, None) is not None\n\nsanity_test_pings = sanity_test_ping_pool.filter(ping_has_sanity_test)\nsanity_test_pings = repartition(sanity_test_pings)\nsanity_test_windows_share = map_x_to_count(sanity_test_pings, 'OSVersion')", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": null, "cell_type": "code", "source": "#########################\n# Sanity test analysis. #\n#########################\ndef GetSanityTests():\n    # Aggregate the sanity test data.\n    SanityTestResults = sanity_test_pings.map(lambda p: p[SANITY_TEST]).reduce(lambda x, y: x + y)\n    \n    reason_pings = sanity_test_pings.filter(lambda p: p.get(SANITY_TEST_REASON, None) is not None)\n    SanityTestReasons = reason_pings.map(lambda p: p[SANITY_TEST_REASON]).reduce(lambda x, y: x + y)\n\n    sanity_test_by_vendor = None\n    sanity_test_by_os = None\n    sanity_test_by_device = None\n    sanity_test_by_driver = None\n    for value in xrange(SANITY_TEST_FAILED_RENDER, SANITY_TEST_LAST_VALUE):\n        subset = sanity_test_pings.filter(lambda p: p[SANITY_TEST][value] > 0)\n        subset = subset.cache()\n\n        tuples = subset.map(lambda p: (value, { p['vendorID']: int(p[SANITY_TEST][value]) }))\n        sanity_test_by_vendor = union_pipelines(sanity_test_by_vendor, tuples)\n    \n        tuples = subset.map(lambda p: (value, { p['OS']: int(p[SANITY_TEST][value]) }))\n        sanity_test_by_os = union_pipelines(sanity_test_by_os, tuples)\n    \n        tuples = subset.map(lambda p: (value, { p['deviceID']: int(p[SANITY_TEST][value]) }))\n        sanity_test_by_device = union_pipelines(sanity_test_by_device, tuples)\n    \n        tuples = subset.map(lambda p: (value, { p['driverShortVersion']: int(p[SANITY_TEST][value]) }))\n        sanity_test_by_driver = union_pipelines(sanity_test_by_driver, tuples)\n    \n    SanityTestByVendor = sanity_test_by_vendor.reduceByKey(combiner)\n    SanityTestByOS = sanity_test_by_os.reduceByKey(combiner)\n    SanityTestByDevice = sanity_test_by_device.reduceByKey(combiner)\n    SanityTestByDriver = sanity_test_by_driver.reduceByKey(combiner)\n    \n    return {\n        'sanityTestPings': sanity_test_pings.count(),\n        'results': [int(value) for value in SanityTestResults],\n        'reasons': [int(value) for value in SanityTestReasons],\n        'byVendor': SanityTestByVendor.collect(),\n        'byOS': SanityTestByOS.collect(),\n        'byDevice': coalesce_to_n_items(SanityTestByDevice.collect(), 10),\n        'byDriver': coalesce_to_n_items(SanityTestByDriver.collect(), 10),\n        'windows': sanity_test_windows_share,\n    }\n    \n# Write Sanity Test statistics.\nTimedExport(filename = 'sanity-test-statistics.json',\n            callback = GetSanityTests,\n            pings = (sanity_test_ping_pool, sanity_test_info))", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": null, "cell_type": "code", "source": "#################################\n# Build sanity test crash data. #\n#################################\ndef make_crash_report(p):\n    obj = {\n        'os': {\n            'name': p['environment/system/os/name'],\n            'version': p.get('environment/system/os/version', None),\n            'servicePack': None,\n        },\n        'adapter': p['environment/system/gfx/adapters'][0],\n        'build': {\n            'version': p['environment/build/version'],\n            'id': p.get('environment/build/buildId', None),\n            'revision': p.get('info/revision', None),\n        },\n        'date': p.get('creationDate', None)\n    }\n    if obj['os']['name'] == 'Windows_NT':\n        spmaj = p.get('environment/system/os/servicePackMajor', 0)\n        spmin = p.get('environment/system/os/servicePackMinor', 0)        \n        if spmaj:\n            if spmin:\n                obj['os']['servicePack'] = '{0}.{1}'.format(spmaj, spmin)\n            else:\n                obj['os']['servicePack'] = '{0}'.format(spmaj)\n    return obj", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": null, "cell_type": "code", "source": "#############################\n# Sanity test crash reports #\n#############################\ndef GetSanityTestCrashes():\n    def ping_has_sanity_test_crash(p):\n        return p[SANITY_TEST][SANITY_TEST_CRASHED] > 0\n\n    sanity_test_crash_pings = sanity_test_pings.filter(ping_has_sanity_test_crash)\n    reports = sanity_test_crash_pings.map(make_crash_report)\n    return {\n        'sanityTestPings': sanity_test_pings.count(),\n        'reports': reports.collect(),\n    }\n    \nTimedExport(filename = 'sanity-test-crash-reports.json',\n            callback = GetSanityTestCrashes,\n            pings = (sanity_test_ping_pool, sanity_test_info))", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": null, "cell_type": "code", "source": "STARTUP_TEST_KEY = 'histograms/GRAPHICS_DRIVER_STARTUP_TEST'\nSTARTUP_OK = 0\nSTARTUP_ENV_CHANGED = 1\nSTARTUP_CRASHED = 2\nSTARTUP_ACCEL_DISABLED = 3\n    \ndef GetStartupTests():\n    startup_test_pings = sanity_test_ping_pool.filter(lambda p: p.get(STARTUP_TEST_KEY, None) is not None)\n    startup_test_pings = startup_test_pings.repartition(MaxPartitions)\n    startup_test_pings = startup_test_pings.cache()\n\n    StartupTestResults = startup_test_pings.map(lambda p: p[STARTUP_TEST_KEY]).reduce(lambda x, y: x + y)\n\n    startup_test_crashes = startup_test_pings.filter(lambda p: p[STARTUP_TEST_KEY][STARTUP_CRASHED] > 0)\n    StartupTestCrashes = startup_test_crashes.map(make_crash_report)\n    \n    return {\n        'startupTestPings': startup_test_pings.count(),\n        'results': [int(i) for i in StartupTestResults],\n        'reports': StartupTestCrashes.collect(),\n        'windows': sanity_test_windows_share,\n    }\n    \n# Write startup test results.\nTimedExport(filename = 'startup-test-statistics.json',\n            callback = GetStartupTests,\n            pings = (sanity_test_ping_pool, sanity_test_info))", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": null, "cell_type": "code", "source": "MonitorsKey = 'environment/system/gfx/monitors'\n\ndef get_monitor_count(p):\n    monitors = p.get(MonitorsKey, None)\n    try:\n        return len(monitors)\n    except:\n        return 0\n    \ndef GetMonitorStatistics():\n    def get_monitor_rdds_for_index(data, i):\n        def get_refresh_rate(p):\n            refreshRate = p[MonitorsKey][i].get('refreshRate', 0)\n            return refreshRate if refreshRate > 1 else 'Unknown'\n        def get_resolution(p):\n            width = p[MonitorsKey][i].get('screenWidth', 0)\n            height = p[MonitorsKey][i].get('screenHeight', 0)\n            if width == 0 or height == 0:\n                return 'Unknown'\n            return '{0}x{1}'.format(width, height)\n    \n        monitors_at_index = data.filter(lambda p: get_monitor_count(p) == monitor_count)\n        monitors_at_index = repartition(monitors_at_index)\n        refresh_rates = monitors_at_index.map(lambda p: (get_refresh_rate(p),))\n        resolutions = monitors_at_index.map(lambda p: (get_resolution(p),))\n        return refresh_rates, resolutions\n    \n    MonitorCounts = sanity_test_ping_pool.map(lambda p: (get_monitor_count(p),)).countByKey()\n    MonitorCounts.pop(0, None)\n\n    refresh_rates = None\n    resolutions = None\n    for monitor_count in MonitorCounts:\n        rate_subset, res_subset = get_monitor_rdds_for_index(sanity_test_ping_pool, monitor_count - 1)\n        refresh_rates = union_pipelines(refresh_rates, rate_subset)\n        resolutions = union_pipelines(resolutions, res_subset)\n    \n    MonitorRefreshRates = refresh_rates.countByKey()\n    MonitorResolutions = resolutions.countByKey()\n    \n    return {\n        'counts': MonitorCounts,\n        'refreshRates': MonitorRefreshRates,\n        'resolutions': MonitorResolutions,\n    }\n\nTimedExport(filename = 'monitor-statistics.json',\n            callback = GetMonitorStatistics,\n            pings = (sanity_test_ping_pool, sanity_test_info))", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": null, "cell_type": "code", "source": "EndTime = datetime.datetime.now()\nTotalElapsed = (EndTime - StartTime).total_seconds()\n\nprint('Total time: {0}'.format(TotalElapsed))", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"collapsed": true, "trusted": true}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "Python 2", "name": "python2", "language": "python"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "version": "2.7.9", "name": "python", "file_extension": ".py", "pygments_lexer": "ipython2", "codemirror_mode": {"version": 2, "name": "ipython"}}}}