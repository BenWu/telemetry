{"nbformat_minor": 0, "cells": [{"execution_count": 1, "cell_type": "code", "source": "import ujson as json\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport plotly.plotly as py\nimport operator\nimport json, time, sys\nfrom __future__ import division\nfrom moztelemetry import get_pings, get_pings_properties, get_one_ping_per_client\ndef fmt_date(d):\n    return d.strftime(\"%Y%m%d\")\ndef repartition(pipeline):\n    return pipeline.repartition(MaxPartitions).cache()\n\n%pylab inline\n\nMaxPartitions = sc.defaultParallelism * 4\nStartTime = datetime.datetime.now()\n\n# Change this to False for local use.\nRUN_AS_TELEMETRY_JOB = True\n\n# Configuration for general data that spans all Firefox versions. Roughly,\n# there are 100mil release channel pings, 7mil beta, 400k aurora, and 180k\n# nightly for the given time window.\n#\n# To not explode the analysis time, the fractions below are adjusted\n# around having ~6mil users total for all channels, or ~3mil total for\n# beta+.\nGeneralTimeWindow = 14\nReleaseFraction = 0.1\nBetaFraction = 0.4\nNightlyAndAuroraFraction = 0.5", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Populating the interactive namespace from numpy and matplotlib\n"}], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": 2, "cell_type": "code", "source": "###########################################################\n# Helper function block for fetching and filtering pings. #\n###########################################################\ndef union_pipelines(a, b):\n    if a is None:\n        return b\n    return a + b\n\n# Helper for scaling back \nclass FeatureDate(object):\n    def __init__(self, availDate, mergeDate):\n        self.availDate = availDate\n        self.mergeDate = mergeDate\n\n        \n    # If a feature has not been available for a given time window yet,\n    # scale up the amount of samples we take to get an equivalent sample\n    # size.\n    def scaleToAvailability(self, startDate, endDate, availDate, fraction):\n        assert availDate > startDate\n        want_days = (endDate - startDate).days\n        avail_days = (endDate - availDate).days\n        fraction = min((fraction * want_days) / avail_days, 1)\n        return availDate, fraction\n    \n    # Try to scale up/down a channel's ping amount based on how long a\n    # feature has been available.\n    def computeChannel(self, startDate, endDate, availDate, fraction):\n        if availDate <= startDate:\n            return {\n                'build_id': (startDate, endDate),\n                'fraction': fraction\n            }\n        \n        newStartDate, newFraction = self.scaleToAvailability(\n            startDate = startDate,\n            endDate = endDate,\n            availDate = availDate,\n            fraction = fraction)\n        return {\n            'build_id': (newStartDate, endDate),\n            'fraction': newFraction\n        }\n    \n    def computeChannels(self, startDate, endDate):\n        channels = {}\n        channels['nightly'] = self.computeChannel(\n            startDate = startDate,\n            endDate = endDate,\n            availDate = self.availDate,\n            fraction = NightlyAndAuroraFraction)\n        \n        # If we haven't merged to aurora, return now.\n        if (endDate - self.mergeDate).days < 1:\n            return channels\n        channels['aurora'] = self.computeChannel(\n            startDate = startDate,\n            endDate = endDate,\n            availDate = self.mergeDate,\n            fraction = NightlyAndAuroraFraction)\n        \n        # Guesstimate the merge date for Beta\n        betaDate = self.mergeDate + datetime.timedelta(6 * 7 + 1)\n        if (endDate - betaDate).days < 1:\n            return channels\n        channels['beta'] = self.computeChannel(\n            startDate = startDate,\n            endDate = endDate,\n            availDate = betaDate,\n            fraction = BetaFraction)\n        \n        # If we've probably released this feature, just select across all\n        # sessions.\n        relDate = betaDate + datetime.timedelta(6 * 7 + 1)\n        if (endDate - relDate).days > 1:\n            return None\n\n        return channels\n    \ndef BuildPingArgs(**kwargs):\n    timeWindow = kwargs.pop('timeWindow', GeneralTimeWindow)\n    channel = kwargs.pop('channel', None)\n    feature = kwargs.pop('feature', None)\n    \n    # Since builds take a bit to disseminate, go back about 4 hour. This is a\n    # completely made up number.\n    limit = datetime.timedelta(0, 60 * 60 * 4)\n    now = datetime.datetime.now()\n    start = now - datetime.timedelta(timeWindow) - limit\n    end = now - limit\n        \n    # Build channel properties.\n    channels = None\n    if channel is None and feature is not None:\n        channels = feature.computeChannels(start, end)\n    elif channel is not None:\n        # Use the old algorithm of build_id.\n        channels = {\n            channel: {\n                'range': (start, end),\n                'fraction': kwargs.pop('fraction'),\n                'build_id': (start, end),\n            }\n        }\n    \n    # If we don't have any channel info, we select across all pings. Here we\n    # use the submission date rather than build_id, since most pings will be\n    # from the release channel where builds are infrequent.\n    if channels is None:\n        channels = {\n            '*': {\n                'submission_date': (start, end),\n                'fraction': kwargs.pop('fraction', ReleaseFraction)\n            }\n        }\n\n    baseArgs = {\n        'app': 'Firefox',\n        'schema': 'v4',\n    }\n    \n    def fmt_dates(tup):\n        return (fmt_date(tup[0]), fmt_date(tup[1]))\n    def day_count(tup):\n        return (tup[1] - tup[0]).days\n\n    metadata = []\n    for channel in channels:\n        info = {}\n        new_args = baseArgs.copy()\n        \n        if channel != '*':\n            new_args['channel'] = channel\n        info['channel'] = channel\n        \n        old_args = channels[channel]\n        \n        new_args['fraction'] = old_args['fraction']\n        info['fraction'] = old_args['fraction']\n            \n        if 'build_id' in old_args:\n            new_args['build_id'] = fmt_dates(old_args['build_id'])\n            info['build_range'] = day_count(old_args['build_id'])\n        if 'submission_date' in old_args:\n            new_args['submission_date'] = fmt_dates(old_args['submission_date'])\n            info['day_range'] = day_count(old_args['submission_date'])\n        \n        metadata.append({\n            'args': new_args,\n            'info': info,\n        })  \n    return metadata\n\ndef FetchRawPings(**kwargs):\n    timestamp = datetime.datetime.now()\n    metadata = BuildPingArgs(**kwargs)\n    pings = None\n    for props in metadata:\n        subset = get_pings(sc, **props['args'])\n        pings = union_pipelines(pings, subset)\n    return pings, {\n        'metadata': metadata,\n        'timestamp': timestamp,\n    }\n        \n# Transform each ping to make it easier to work with in later stages.\ndef Validate(p):\n    name = p.get(\"environment/system/os/name\") or 'w'\n    version = p.get(\"environment/system/os/version\") or '0'\n    if name == 'Linux':\n        p['OSVersion'] = None\n        p['OS'] = 'Linux'\n        p['OSName'] = 'Linux'\n    elif name == 'Windows_NT':\n        spmaj = p.get(\"environment/system/os/servicePackMajor\") or '0'\n        p['OSVersion'] = version + '.' + str(spmaj)\n        p['OS'] = 'Windows-' + version + '.' + str(spmaj)\n        p['OSName'] = 'Windows'\n    elif name == 'Darwin':\n        p['OSVersion'] = version\n        p['OS'] = 'Darwin-' + version\n        p['OSName'] = 'Darwin'\n    else:\n        p['OSVersion'] = version\n        p['OS'] = '{0}-{1}'.format(name, version)\n        p['OSName'] = name\n    \n    # Telemetry data isn't guaranteed to be well-formed so unfortunately\n    # we have to do some validation on it. If we get to the end, we set\n    # p['valid'] to True, and this gets filtered over later. In addition\n    # we have a wrapper below to help fetch strings that may be null.\n    if not p.get(\"environment/build/version\", None):\n        return p\n    p['FxVersion'] = p[\"environment/build/version\"].split('.')[0]\n    \n    # Verify that we have at least one adapter.\n    try:\n        adapter = p[\"environment/system/gfx/adapters\"][0]\n    except:\n        return p\n    if adapter is None or not hasattr(adapter, '__getitem__'):\n        return p\n    \n    def T(obj, key):\n        return obj.get(key, None) or 'Unknown'\n    \n    # We store the device ID as a vendor/device string, because the device ID\n    # alone is not enough to determine whether the key is unique.\n    #\n    # We also merge 'Intel Open Source Technology Center' with the device ID\n    # that should be reported, 0x8086, for simplicity.\n    vendorID = T(adapter, 'vendorID')\n    if vendorID == u'Intel Open Source Technology Center':\n        p['vendorID'] = u'0x8086'\n    else:\n        p['vendorID'] = vendorID\n    p['deviceID'] = u'{0}/{1}'.format(p['vendorID'], T(adapter, 'deviceID'))\n    p['driverVersion'] = u'{0}/{1}'.format(p['vendorID'], T(adapter, 'driverVersion'))\n    p['deviceAndDriver'] = u'{0}/{1}'.format(p['deviceID'], T(adapter, 'driverVersion'))\n        \n    p['valid'] = True\n    return p\n\ndef reduce_pings(pings):\n    return get_pings_properties(pings, [\n      'clientId',\n      \"creationDate\",\n      \"environment/settings/userPrefs\",\n      \"environment/build/architecture\",\n      \"environment/build/version\",\n      \"environment/build/buildId\",\n      \"environment/system/memoryMB\",\n      \"environment/system/isWow64\",\n      \"environment/system/cpu\",\n      \"environment/system/os/name\",\n      \"environment/system/os/version\",\n      \"environment/system/os/servicePackMajor\",\n      \"environment/system/os/servicePackMinor\",\n      \"environment/system/gfx/adapters\",\n      \"environment/system/gfx/features\",\n      \"environment/system/gfx/monitors\",\n      \"payload/histograms/DEVICE_RESET_REASON\",\n      \"payload/histograms/GRAPHICS_SANITY_TEST\",\n      \"payload/histograms/GRAPHICS_SANITY_TEST_REASON\",\n      \"payload/histograms/GRAPHICS_DRIVER_STARTUP_TEST\",\n      \"payload/info/revision\",\n    ])\n\ndef FormatPings(pings):\n    pings = reduce_pings(pings)\n    pings = get_one_ping_per_client(pings)\n    pings = pings.map(Validate)\n    filtered_pings = pings.filter(lambda p: p.get('valid', False) == True)\n    return filtered_pings.cache()\n\ndef FetchAndFormat(**kwargs):\n    raw_pings, info = FetchRawPings(**kwargs)\n    return FormatPings(raw_pings), info", "outputs": [], "metadata": {"collapsed": true, "trusted": true}}, {"execution_count": 3, "cell_type": "code", "source": "##################################################################\n# Helper function block for massaging pings into aggregate data. #\n##################################################################\n\n# Take each key in |b| and add it to |a|, accumulating its value into\n# |a| if it already exists.\ndef combiner(a, b):\n    result = a\n    for key in b:\n        countA = a.get(key, 0)\n        countB = b[key]\n        result[key] = countA + countB\n    return result\n\n# Same as combiner(), but does not mutate either input.\ndef safe_combiner(left, right):\n    result = left.copy()\n    for key in right:\n        result[key] = result.get(key, 0) + right[key]\n    return result\n\n# Return an aggregation based on combiner.\ndef aggregation(data, fn):\n    view = data.map(fn)\n    return view.reduceByKey(combiner)\n\n# Helper for reduceByKey.\ndef map_x_to_y(data, sourceKey, destKey):\n    def extract(p):\n        return (p[sourceKey], { p[destKey]: 1 })\n    return aggregation(data, extract)\n\n# Helper for reduceByKey => count.\ndef map_x_to_count(data, sourceKey):\n    def extract(p):\n        return (p[sourceKey],)\n    return data.map(extract).countByKey()\n\n# After reduceByKey(combiner), we get a mapping like:\n#  key => { variable => value }\n#\n# This function collapses 'variable' instances below a threshold into\n# a catch-all identifier ('Other').\ndef coalesce_to_n_items(agg, max_items):\n    obj = []\n    for superkey, breakdown in agg:\n        if len(breakdown) <= max_items:\n            obj += [(superkey, breakdown)]\n            continue\n        items = sorted(breakdown.items(), key=lambda obj: obj[1], reverse=True)\n        new_breakdown = {k: v for k, v in items[0:max_items]}\n        total = 0\n        for k, v in items[max_items:]:\n            total += v\n        if total:\n            new_breakdown['Other'] = new_breakdown.get('Other', 0) + total\n        obj += [(superkey, new_breakdown)]\n    return obj", "outputs": [], "metadata": {"collapsed": true, "trusted": true}}, {"execution_count": 4, "cell_type": "code", "source": "#############################\n# Helper for writing files. #\n#############################\n\ndef ApplyPingInfo(obj, **kwargs):\n    if 'pings' not in kwargs:\n        return\n    \n    pings, info = kwargs.pop('pings')\n    \n    # To make the sample source information more transparent, we include\n    # the breakdown of Firefox channel numbers.\n    if '__share' not in info:\n        info['__share'] = map_x_to_count(pings, 'FxVersion')\n    \n    obj['sessions'] = {\n        'count': pings.count(),\n        'timestamp': time.mktime(info['timestamp'].timetuple()),\n        'metadata': info['metadata'],\n        'share': info['__share'],\n    }\n    \ndef Export(filename, obj):\n    if RUN_AS_TELEMETRY_JOB:\n        filename = os.path.join('output', filename)\n    with open(filename, 'w') as fp:\n        json.dump(obj, fp)\n        \ndef TimedExport(filename, callback, **kwargs):\n    start = datetime.datetime.now()\n    \n    obj = callback()\n    ApplyPingInfo(obj, **kwargs)\n    \n    end = datetime.datetime.now()\n    elapsed = end - start\n    obj['phaseTime'] = elapsed.total_seconds()\n    \n    Export(filename, obj)\n    export_time = datetime.datetime.now() - end\n    \n    print('Computed {0} in {1} seconds.'.format(filename, elapsed.total_seconds()))\n    print('Exported {0} in {1} seconds.'.format(filename, export_time.total_seconds()))\n    \n# Profiler for debugging.\nclass Prof(object):\n    def __init__(self, name):\n        self.name = name\n    def __enter__(self):\n        self.sout('Starting {0}... '.format(self.name))\n        self.start = datetime.datetime.now()\n        return None\n    def __exit__(self, type, value, traceback):\n        self.end = datetime.datetime.now()\n        self.sout('... {0}: {1}s'.format(self.name, (self.end - self.start).total_seconds()))\n    def sout(self, s):\n        sys.stdout.write(s)\n        sys.stdout.write('\\n')\n        sys.stdout.flush()", "outputs": [], "metadata": {"collapsed": true, "trusted": true}}, {"execution_count": 5, "cell_type": "code", "source": "def quiet_logs(sc):\n  logger = sc._jvm.org.apache.log4j\n  logger.LogManager.getLogger(\"org\").setLevel(logger.Level.ERROR)\n  logger.LogManager.getLogger(\"akka\").setLevel(logger.Level.ERROR)\nquiet_logs(sc)", "outputs": [], "metadata": {"collapsed": true, "trusted": true}}, {"execution_count": 6, "cell_type": "code", "source": "# Get a general ping sample across all Firefox channels.\nwith Prof(\"General pings\") as px:\n    GeneralPings, GeneralPingInfo  = FetchAndFormat()\n    GeneralPings = GeneralPings.cache()\n\n    # Windows gets some preferential breakdown treatment.\n    WindowsPings = GeneralPings.filter(lambda p: p['OSName'] == 'Windows')\n    WindowsPings = WindowsPings.cache()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Starting General pings... \n... General pings: 556.190044s\n"}], "metadata": {"scrolled": false, "collapsed": false, "trusted": true}}, {"execution_count": 7, "cell_type": "code", "source": "# Results by operating system.\nif '__share' not in GeneralPingInfo:\n    GeneralPingInfo['__share'] = map_x_to_count(GeneralPings, 'FxVersion')\n\ndef GetGeneralStatisticsForSubset(subset, windows_subset):\n    OSShare = map_x_to_count(subset, 'OSName')\n\n    # Results by Windows version.\n    WindowsShare = map_x_to_count(windows_subset, 'OSVersion')\n\n    # Top-level stats.\n    VendorShare = map_x_to_count(subset, 'vendorID')\n    \n    return {\n        'os': OSShare,\n        'windows': WindowsShare,\n        'vendors': VendorShare,\n    }\n\ndef GetGeneralStatistics():\n    obj = {}\n    obj['devices'] = map_x_to_count(GeneralPings, 'deviceID')\n    obj['drivers'] = map_x_to_count(GeneralPings, 'driverVersion')\n    \n    byFx = {}\n    with Prof('general stats for all') as px:\n        byFx['all'] = GetGeneralStatisticsForSubset(GeneralPings, WindowsPings)\n    \n    for key in GeneralPingInfo['__share']:\n        subset = GeneralPings.filter(lambda p: p['FxVersion'] == key)\n        windows = subset.filter(lambda p: p['OSName'] == 'Windows')\n        subset = repartition(subset)\n        windows = repartition(windows)\n        with Prof('general stats for ' + key) as px:\n            byFx[key] = GetGeneralStatisticsForSubset(subset, windows)\n        \n    obj['byFx'] = byFx\n    return obj\n        \nTimedExport(filename = 'general-statistics.json',\n            callback = GetGeneralStatistics,\n            pings = (GeneralPings, GeneralPingInfo))", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Starting general stats for all... \n... general stats for all: 420.359319s\nStarting general stats for 39... \n... general stats for 39: 408.813486s\nStarting general stats for 44... \n... general stats for 44: 407.544638s\nStarting general stats for 45... \n... general stats for 45: 424.198493s\nStarting general stats for 42... \n... general stats for 42: 428.049759s\nStarting general stats for 43... \n... general stats for 43: 431.628391s\nStarting general stats for 40... \n... general stats for 40: 430.354483s\nStarting general stats for 41... \n... general stats for 41: 440.532465s\nComputed general-statistics.json in 3822.545237 seconds.\nExported general-statistics.json in 0.03137 seconds.\n"}], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": 8, "cell_type": "code", "source": "def GetDriverStatistics():\n    obj = {}\n    obj['deviceAndDriver'] = map_x_to_count(GeneralPings,'deviceAndDriver')\n    return obj\n\nTimedExport(filename = 'device-statistics.json',\n            callback = GetDriverStatistics,\n            pings = (GeneralPings, GeneralPingInfo))", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Computed device-statistics.json in 298.913387 seconds.\nExported device-statistics.json in 0.204228 seconds.\n"}], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": 9, "cell_type": "code", "source": "DeviceResetReasonKey = 'payload/histograms/DEVICE_RESET_REASON'\n\n#############################\n# Perform the TDR analysis. #\n#############################\ndef GetTDRStatistics():\n    NumTDRReasons = 8\n    def ping_has_tdr_for(p, reason):\n        return p[DeviceResetReasonKey][reason] > 0\n\n    # Specialized version of map_x_to_y, for TDRs. We cast to int because for\n    # some reason the values Spark returns do not serialize with JSON.\n    def map_reason_to_vendor(p, reason, destKey):\n        return (int(reason), { p[destKey]: int(p[DeviceResetReasonKey][reason]) })\n    def map_vendor_to_reason(p, reason, destKey):\n        return (p[destKey], { int(reason): int(p[DeviceResetReasonKey][reason]) })\n\n    # Filter out pings that do not have any TDR data. We expect this to be a huge reduction\n    # in the sample set, and the resulting partition count gets way off. We repartition\n    # immediately for performance.\n    TDRSubset = WindowsPings.filter(lambda p: p.get(DeviceResetReasonKey, None) is not None)\n    TDRSubset = TDRSubset.repartition(MaxPartitions)\n    TDRSubset = TDRSubset.cache()\n\n    # Aggregate the device reset data.\n    TDRResults = TDRSubset.map(lambda p: p[DeviceResetReasonKey]).reduce(lambda x, y: x + y)\n\n    # For each TDR reason, get a list tuple of (reason, vendor => resetCount). Then\n    # we combine these into a single series.\n    reason_to_vendor_tuples = None\n    vendor_to_reason_tuples = None\n    for reason in xrange(1, NumTDRReasons):\n        subset = TDRSubset.filter(lambda p: ping_has_tdr_for(p, reason))\n        subset = subset.cache()\n\n        tuples = subset.map(lambda p: map_reason_to_vendor(p, reason, 'vendorID'))\n        reason_to_vendor_tuples = union_pipelines(reason_to_vendor_tuples, tuples)\n\n        tuples = subset.map(lambda p: map_vendor_to_reason(p, reason, 'vendorID'))\n        vendor_to_reason_tuples = union_pipelines(vendor_to_reason_tuples, tuples)\n\n    TDRReasonToVendor = reason_to_vendor_tuples.reduceByKey(combiner, MaxPartitions)\n    TDRVendorToReason = vendor_to_reason_tuples.reduceByKey(combiner, MaxPartitions)\n    \n    return {\n        'tdrPings': TDRSubset.count(),\n        'results': [int(value) for value in TDRResults],\n        'reasonToVendor': TDRReasonToVendor.collect(),\n        'vendorToReason': TDRVendorToReason.collect(),\n    }\n    \n# Write TDR statistics.\nTimedExport(filename = 'tdr-statistics.json',\n            callback = GetTDRStatistics,\n            pings = (WindowsPings, GeneralPingInfo))", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Computed tdr-statistics.json in 547.236235 seconds.\nExported tdr-statistics.json in 0.003545 seconds.\n"}], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": 10, "cell_type": "code", "source": "##########################\n# Get system statistics. #\n##########################\n\nArchKey = 'environment/build/architecture'\nCpuKey = 'environment/system/cpu'\nMemoryKey = 'environment/system/memoryMB'\n\ndef GetBucketedMemory(pings): \n    def get_bucket(p):\n        x = int(p / 1000)\n        if x < 1:\n            return 'less_1gb'\n        if x <= 4:\n            return x\n        if x <= 8:\n            return '4_to_8'\n        if x <= 16:\n            return '8_to_16'\n        if x <= 32:\n            return '16_to_32'\n        return 'more_32'\n    \n    memory_rdd = pings.map(lambda p: p.get(MemoryKey, 0))\n    memory_rdd = memory_rdd.filter(lambda p: p > 0)\n    memory_rdd = memory_rdd.map(lambda p: (get_bucket(p),))\n    memory_rdd = repartition(memory_rdd)\n    return memory_rdd.countByKey()\n\ndef GetCpuFeatures(pings):\n    cpuid_rdd = pings.map(lambda p: p.get(CpuKey, None))\n    cpuid_rdd = cpuid_rdd.filter(lambda p: p is not None)\n    cpuid_rdd = cpuid_rdd.map(lambda p: p.get('extensions', None))\n    \n    # Unfortunately, Firefox 39 had a bug where CPU features could be reported even\n    # if they weren't present. To detect this we filter pings that have ARMv6 support\n    # on x86/64.\n    cpuid_rdd = cpuid_rdd.filter(lambda p: p is not None and 'hasARMv6' not in p)\n    cpuid_rdd = repartition(cpuid_rdd)\n\n    # Count before we blow up the list.\n    with Prof('cpu count for x86') as px:\n        total = cpuid_rdd.count()\n\n    cpuid_rdd = cpuid_rdd.flatMap(lambda p: [(ex, 1) for ex in p])\n    with Prof('cpu features for x86') as px:\n        feature_map = cpuid_rdd.countByKey()\n        \n    return {\n        'total': total,\n        'features': feature_map,\n    }\n\ndef GetSystemStatistics():\n    def get_logical_cores(p):\n        cpu = p.get(CpuKey, None)\n        if cpu is None:\n            return 'unknown'\n        return cpu.get('count', 'unknown')\n    with Prof('logical cores') as px:\n        logical_cores = GeneralPings.map(lambda p: (get_logical_cores(p),)).countByKey()\n    \n    cpu_features = GetCpuFeatures(GeneralPings)\n    \n    with Prof('memory buckets') as px:\n        memory = GetBucketedMemory(GeneralPings)\n        \n    def get_os_bits(p):\n        arch = p.get(ArchKey, 'unknown')\n        if arch == 'x86-64':\n            return '64'\n        if arch == 'x86':\n            wow64 = p.get(\"environment/system/isWow64\", False)\n            if wow64:\n                return '32_on_64'\n            return '32'\n        return 'unknown'\n    \n    with Prof('OS bit count') as px:\n        os_bits = WindowsPings.map(lambda p: (get_os_bits(p),)).countByKey()\n    \n    return {\n        'logical_cores': logical_cores,\n        'x86': cpu_features,\n        'memory': memory,\n        'wow': os_bits,\n    }\n\nTimedExport(filename = 'system-statistics.json',\n            callback = GetSystemStatistics,\n            pings = (GeneralPings, GeneralPingInfo))", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Starting logical cores... \n... logical cores: 146.183732s\nStarting cpu count for x86... \n... cpu count for x86: 154.327415s\nStarting cpu features for x86... \n... cpu features for x86: 80.069669s\nStarting memory buckets... \n... memory buckets: 154.43035s\nStarting OS bit count... \n... OS bit count: 199.891968s\nComputed system-statistics.json in 882.528412 seconds.\nExported system-statistics.json in 0.000607 seconds.\n"}], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": null, "cell_type": "code", "source": "# Done with global pings.\nWindowsPings = None\nGeneralPings = None\nGeneralPingInfo = None", "outputs": [], "metadata": {"collapsed": true, "trusted": true}}, {"execution_count": null, "cell_type": "code", "source": "# Get pings with graphics features. This landed in roughly the 7-19-2015 nightly.\nFeaturesKey = 'environment/system/gfx/features'\nUserPrefsKey = 'environment/settings/userPrefs'\n\nWindowsFeaturePings, WindowsFeaturePingsInfo = FetchAndFormat(\n    feature = FeatureDate(\n        availDate = datetime.datetime(2015, 7, 20),\n        mergeDate = datetime.datetime(2015, 8, 10))\n)\n\ndef windows_feature_filter(p):\n    return p['OSName'] == 'Windows' and p.get(FeaturesKey) is not None\n\nWindowsFeatures = WindowsFeaturePings.filter(windows_feature_filter)\nWindowsFeatures = WindowsFeatures.cache()", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": null, "cell_type": "code", "source": "# Build graphics feature statistics.\ndef get_compositor(p):\n    compositor = p[FeaturesKey].get('compositor', 'none')\n    if compositor == 'none':\n        userPrefs = p.get(UserPrefsKey, None)\n        if userPrefs is not None:\n            omtc = userPrefs.get('layers.offmainthreadcomposition.enabled', True)\n            if omtc != True:\n                compositor = 'disabled'\n    return (compositor,)\n\ndef get_d3d11_status(p):\n    d3d11 = p[FeaturesKey].get('d3d11', None)\n    if not hasattr(d3d11, '__getitem__'):\n        return 'unknown'\n    status = d3d11.get('status', 'unknown')\n    if status != 'available':\n        return status\n    if d3d11.get('warp', False) == True:\n        return 'warp'\n    return d3d11.get('version', 'unknown')\n\ndef get_warp_status(p):\n    if 'blacklisted' not in p[FeaturesKey]['d3d11']:\n        return 'unknown'\n    if p[FeaturesKey]['d3d11']['blacklisted'] == True:\n        return 'blacklist'\n    return 'device failure'\n\ndef get_d2d_status(p):\n    d2d = p[FeaturesKey].get('d2d', None)\n    if not hasattr(d2d, '__getitem__'):\n        return ('unknown',)\n    status = d2d.get('status', 'unknown')\n    if status != 'available':\n        return (status,)\n    return (d2d.get('version', 'unknown'),)\n\ndef has_working_d3d11(p):\n    d3d11 = p[FeaturesKey].get('d3d11', None)\n    if d3d11 is None:\n        return False\n    return d3d11.get('status') == 'available'\n\ndef get_texture_sharing_status(p):\n    return (p[FeaturesKey]['d3d11'].get('textureSharing', 'unknown'),)\n\n# We skip certain windows versions in detail lists since this phase is\n# very expensive to compute.\nImportantWindowsVersions = (\n    '5.1.2',\n    '5.1.3',\n    '6.0.2',\n    '6.1.0',\n    '6.1.1',\n    '6.2.0',\n    '6.3.0',\n    '10.0.0',\n)\n\ndef GetWindowsFeatures():\n    WindowsCompositorMap = WindowsFeatures.map(get_compositor).countByKey()\n    D3D11StatusMap = WindowsFeatures.map(lambda p: (get_d3d11_status(p),)).countByKey()\n    D2DStatusMap = WindowsFeatures.map(get_d2d_status).countByKey()\n    \n    warp_pings = WindowsFeatures.filter(lambda p: get_d3d11_status(p) == 'warp')\n    warp_pings = repartition(warp_pings)\n    WarpStatusMap = warp_pings.map(lambda p: (get_warp_status(p),)).countByKey()\n\n    TextureSharingMap = WindowsFeatures.filter(has_working_d3d11).map(get_texture_sharing_status).countByKey()\n\n    # Now, build the same data except per version.\n    feature_pings_by_os = map_x_to_count(WindowsFeatures, 'OSVersion')\n    WindowsFeaturesByVersion = {}\n    for os_version in feature_pings_by_os:\n        if os_version not in ImportantWindowsVersions:\n            continue\n        subset = WindowsFeatures.filter(lambda p: p['OSVersion'] == os_version)\n        subset = repartition(subset)\n        \n        results = {\n            'count': subset.count(),\n            'compositors': subset.map(get_compositor).countByKey(),\n        }\n        try:\n            if int(os_version.split('.')[0]) >= 6:\n                results['d3d11'] = subset.map(lambda p: (get_d3d11_status(p),)).countByKey()\n                results['d2d'] = subset.map(get_d2d_status).countByKey()\n                \n                warp_pings = subset.filter(lambda p: get_d3d11_status(p) == 'warp')\n                results['warp'] = warp_pings.map(lambda p: (get_warp_status(p),)).countByKey()\n        except:\n            pass\n        finally:\n            # Free resources.\n            warp_pings = None\n            subset = None\n        WindowsFeaturesByVersion[os_version] = results\n    \n    return {\n        'all': {\n            'compositors': WindowsCompositorMap,\n            'd3d11': D3D11StatusMap,\n            'd2d': D2DStatusMap,\n            'textureSharing': TextureSharingMap,\n            'warp': WarpStatusMap,\n        },\n        'byVersion': WindowsFeaturesByVersion,\n    }\n\nTimedExport(filename = 'windows-features.json',\n            callback = GetWindowsFeatures,\n            pings = (WindowsFeatures, WindowsFeaturePingsInfo))", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": null, "cell_type": "code", "source": "WindowsFeatures = None\nWindowsFeaturePings = None\nWindowsFeaturePingsInfo = None", "outputs": [], "metadata": {"collapsed": true, "trusted": true}}, {"execution_count": null, "cell_type": "code", "source": "# Sanity test and crash analysis setup.\n# The sanity test was only available in Firefox 41 and higher, so currently\n# our analyses grabs a different sample of pings to be more accurate.\n#\n# Once the sanity test makes it to beta we can use the main data set.\nsanity_test_ping_pool, sanity_test_info = FetchAndFormat(\n    feature = FeatureDate(\n        availDate = datetime.datetime(2015, 6, 25),\n        mergeDate = datetime.datetime(2015, 6, 29))\n)\n\n# Set up constants.\nSANITY_TEST_PASSED = 0\nSANITY_TEST_FAILED_RENDER = 1\nSANITY_TEST_FAILED_VIDEO = 2\nSANITY_TEST_CRASHED = 3\nSANITY_TEST_TIMEDOUT = 4\nSANITY_TEST_LAST_VALUE = 5\nSANITY_TEST_REASON_FIRST_RUN = 0\nSANITY_TEST_REASON_FIREFOX_CHANGED = 1\nSANITY_TEST_REASON_DEVICE_CHANGED = 2\nSANITY_TEST_REASON_DRIVER_CHANGED = 3\nSANITY_TEST_REASON_LAST_VALUE = 4\n\nSANITY_TEST = 'payload/histograms/GRAPHICS_SANITY_TEST'\nSANITY_TEST_REASON = 'payload/histograms/GRAPHICS_SANITY_TEST_REASON'", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": null, "cell_type": "code", "source": "# Filter sanity test pings.\ndef ping_has_sanity_test(p):\n    return p.get(SANITY_TEST, None) is not None", "outputs": [], "metadata": {"collapsed": true, "trusted": true}}, {"execution_count": null, "cell_type": "code", "source": "#########################\n# Sanity test analysis. #\n#########################\ndef GetSanityTestsForSlice(filter_func, is_xp = False):\n    sanity_test_pings = sanity_test_ping_pool.filter(filter_func)\n    data = sanity_test_pings.filter(ping_has_sanity_test)\n    \n    # We don't want to fold FAILED_LAYERS and FAILED_VIDEO into the same\n    # resultset, so we use this function to split them out.\n    def get_sanity_test_result(p):\n        if p[SANITY_TEST][SANITY_TEST_PASSED] > 0:\n            return SANITY_TEST_PASSED\n        if p[SANITY_TEST][SANITY_TEST_CRASHED] > 0:\n            return SANITY_TEST_CRASHED\n        if p[SANITY_TEST][SANITY_TEST_FAILED_RENDER] > 0:\n            return SANITY_TEST_FAILED_RENDER\n        if p[SANITY_TEST][SANITY_TEST_FAILED_VIDEO] > 0:\n            return SANITY_TEST_FAILED_VIDEO\n        return SANITY_TEST_LAST_VALUE\n    \n    # Aggregate the sanity test data.\n    with Prof('initial map') as px:\n        SanityTestResults = data.map(lambda p: (get_sanity_test_result(p),)).countByKey()\n\n    with Prof('share resolve') as px:\n        os_share = map_x_to_count(data, 'OSVersion')\n        \n    with Prof('ping_count') as px:\n        sanity_test_count = data.count()\n        ping_count = sanity_test_pings.count()\n\n    sanity_test_by_vendor = None\n    sanity_test_by_os = None\n    sanity_test_by_device = None\n    sanity_test_by_driver = None\n    for value in xrange(SANITY_TEST_FAILED_RENDER, SANITY_TEST_LAST_VALUE):\n        if is_xp and value == SANITY_TEST_FAILED_VIDEO:\n            continue\n\n        subset = data.filter(lambda p: get_sanity_test_result(p) == value)\n\n        tuples = subset.map(lambda p: (value, { p['vendorID']: int(p[SANITY_TEST][value]) }))\n        sanity_test_by_vendor = union_pipelines(sanity_test_by_vendor, tuples)\n    \n        tuples = subset.map(lambda p: (value, { p['OS']: int(p[SANITY_TEST][value]) }))\n        sanity_test_by_os = union_pipelines(sanity_test_by_os, tuples)\n    \n        tuples = subset.map(lambda p: (value, { p['deviceID']: int(p[SANITY_TEST][value]) }))\n        sanity_test_by_device = union_pipelines(sanity_test_by_device, tuples)\n    \n        tuples = subset.map(lambda p: (value, { p['driverVersion']: int(p[SANITY_TEST][value]) }))\n        sanity_test_by_driver = union_pipelines(sanity_test_by_driver, tuples)\n            \n    sanity_test_by_vendor = repartition(sanity_test_by_vendor)\n    sanity_test_by_os = repartition(sanity_test_by_os)\n    sanity_test_by_device = repartition(sanity_test_by_device)\n    sanity_test_by_driver = repartition(sanity_test_by_driver)\n    \n    with Prof('vendor resolve') as px:\n        SanityTestByVendor = sanity_test_by_vendor.reduceByKey(combiner)\n    with Prof('os resolve') as px:\n        SanityTestByOS = sanity_test_by_os.reduceByKey(combiner)\n    with Prof('device resolve') as px:\n        SanityTestByDevice = sanity_test_by_device.reduceByKey(combiner)\n    with Prof('driver resolve') as px:\n        SanityTestByDriver = sanity_test_by_driver.reduceByKey(combiner)\n        \n    print('Partitions: {0},{1},{2},{3}'.format(\n        SanityTestByVendor.getNumPartitions(),\n        SanityTestByOS.getNumPartitions(),\n        SanityTestByDevice.getNumPartitions(),\n        SanityTestByDriver.getNumPartitions()))\n        \n    with Prof('vendor collect') as px:\n        byVendor = SanityTestByVendor.collect()\n    with Prof('os collect') as px:\n        byOS = SanityTestByOS.collect()\n    with Prof('device collect') as px:\n        byDevice = SanityTestByDevice.collect()\n    with Prof('driver collect') as px:\n        byDriver = SanityTestByDriver.collect()\n    \n    return {\n        'sanityTestPings': sanity_test_count,\n        'totalPings': ping_count,\n        'results': SanityTestResults,\n        'byVendor': byVendor,\n        'byOS': byOS,\n        'byDevice': coalesce_to_n_items(byDevice, 10),\n        'byDriver': coalesce_to_n_items(byDriver, 10),\n        'windows': os_share,\n    }\n\ndef GetSanityTests(): \n    obj = {}\n    \n    def windows_xp(p):\n        return p['OSName'] == 'Windows' and p['OSVersion'].startswith('5.')\n    obj['windowsXP'] = GetSanityTestsForSlice(windows_xp, is_xp = True)\n    \n    def windows_vista(p):\n        return p['OSName'] == 'Windows' and not p['OSVersion'].startswith('5.')\n    obj['windows'] = GetSanityTestsForSlice(windows_vista)\n    \n    return obj\n    \n# Write Sanity Test statistics.\nTimedExport(filename = 'sanity-test-statistics.json',\n            callback = GetSanityTests,\n            pings = (sanity_test_ping_pool, sanity_test_info))", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": null, "cell_type": "code", "source": "#################################\n# Build sanity test crash data. #\n#################################\ndef make_crash_report(p):\n    obj = {\n        'os': {\n            'name': p['environment/system/os/name'],\n            'version': p.get('environment/system/os/version', None),\n            'servicePack': None,\n        },\n        'adapter': p['environment/system/gfx/adapters'][0],\n        'build': {\n            'version': p['environment/build/version'],\n            'id': p.get('environment/build/buildId', None),\n            'revision': p.get('payload/info/revision', None),\n        },\n        'date': p.get('creationDate', None)\n    }\n    if obj['os']['name'] == 'Windows_NT':\n        spmaj = p.get('environment/system/os/servicePackMajor', 0)\n        spmin = p.get('environment/system/os/servicePackMinor', 0)        \n        if spmaj:\n            if spmin:\n                obj['os']['servicePack'] = '{0}.{1}'.format(spmaj, spmin)\n            else:\n                obj['os']['servicePack'] = '{0}'.format(spmaj)\n    return obj", "outputs": [], "metadata": {"collapsed": true, "trusted": true}}, {"execution_count": null, "cell_type": "code", "source": "#############################\n# Sanity test crash reports #\n#############################\ndef GetSanityTestCrashes():\n    def ping_has_sanity_test_crash(p):\n        return p[SANITY_TEST][SANITY_TEST_CRASHED] > 0\n\n    sanity_test_pings = sanity_test_ping_pool.filter(ping_has_sanity_test)\n    sanity_test_pings = repartition(sanity_test_pings)\n    sanity_test_crash_pings = sanity_test_pings.filter(ping_has_sanity_test_crash)\n    reports = sanity_test_crash_pings.map(make_crash_report)\n    return {\n        'sanityTestPings': sanity_test_pings.count(),\n        'reports': reports.collect(),\n    }\n    \nTimedExport(filename = 'sanity-test-crash-reports.json',\n            callback = GetSanityTestCrashes,\n            pings = (sanity_test_ping_pool, sanity_test_info))", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": null, "cell_type": "code", "source": "STARTUP_TEST_KEY = 'payload/histograms/GRAPHICS_DRIVER_STARTUP_TEST'\nSTARTUP_OK = 0\nSTARTUP_ENV_CHANGED = 1\nSTARTUP_CRASHED = 2\nSTARTUP_ACCEL_DISABLED = 3\n    \ndef GetStartupTests():\n    startup_test_pings = sanity_test_ping_pool.filter(lambda p: p.get(STARTUP_TEST_KEY, None) is not None)\n    startup_test_pings = startup_test_pings.repartition(MaxPartitions)\n    startup_test_pings = startup_test_pings.cache()\n\n    StartupTestResults = startup_test_pings.map(lambda p: p[STARTUP_TEST_KEY]).reduce(lambda x, y: x + y)\n\n    startup_test_crashes = startup_test_pings.filter(lambda p: p[STARTUP_TEST_KEY][STARTUP_CRASHED] > 0)\n    StartupTestCrashes = startup_test_crashes.map(make_crash_report)\n    \n    os_share = map_x_to_count(startup_test_pings, 'OS')\n    \n    return {\n        'startupTestPings': startup_test_pings.count(),\n        'results': [int(i) for i in StartupTestResults],\n        'reports': StartupTestCrashes.collect(),\n        'windows': os_share,\n    }\n    \n# Write startup test results.\nTimedExport(filename = 'startup-test-statistics.json',\n            callback = GetStartupTests,\n            pings = (sanity_test_ping_pool, sanity_test_info))", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": null, "cell_type": "code", "source": "MonitorsKey = 'environment/system/gfx/monitors'\n\ndef get_monitor_count(p):\n    monitors = p.get(MonitorsKey, None)\n    try:\n        return len(monitors)\n    except:\n        return 0\n    \ndef get_monitor_res(p, i):\n    width = p[MonitorsKey][i].get('screenWidth', 0)\n    height = p[MonitorsKey][i].get('screenHeight', 0)\n    if width == 0 or height == 0:\n        return 'Unknown'\n    return '{0}x{1}'.format(width, height)", "outputs": [], "metadata": {"collapsed": true, "trusted": true}}, {"execution_count": null, "cell_type": "code", "source": "def GetMonitorStatistics():\n    def get_monitor_rdds_for_index(data, i):\n        def get_refresh_rate(p):\n            refreshRate = p[MonitorsKey][i].get('refreshRate', 0)\n            return refreshRate if refreshRate > 1 else 'Unknown'\n        def get_resolution(p):\n            return get_monitor_res(p, i)\n    \n        monitors_at_index = data.filter(lambda p: get_monitor_count(p) == monitor_count)\n        monitors_at_index = repartition(monitors_at_index)\n        refresh_rates = monitors_at_index.map(lambda p: (get_refresh_rate(p),))\n        resolutions = monitors_at_index.map(lambda p: (get_resolution(p),))\n        return refresh_rates, resolutions\n    \n    MonitorCounts = sanity_test_ping_pool.map(lambda p: (get_monitor_count(p),)).countByKey()\n    MonitorCounts.pop(0, None)\n\n    refresh_rates = None\n    resolutions = None\n    for monitor_count in MonitorCounts:\n        rate_subset, res_subset = get_monitor_rdds_for_index(sanity_test_ping_pool, monitor_count - 1)\n        refresh_rates = union_pipelines(refresh_rates, rate_subset)\n        resolutions = union_pipelines(resolutions, res_subset)\n    \n    MonitorRefreshRates = refresh_rates.countByKey()\n    MonitorResolutions = resolutions.countByKey()\n    \n    return {\n        'counts': MonitorCounts,\n        'refreshRates': MonitorRefreshRates,\n        'resolutions': MonitorResolutions,\n    }\n\nTimedExport(filename = 'monitor-statistics.json',\n            callback = GetMonitorStatistics,\n            pings = (sanity_test_ping_pool, sanity_test_info))", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": null, "cell_type": "code", "source": "def GetAPZStatistics(pings):\n    def apz_enabled(p):\n        userPrefs = p.get(UserPrefsKey)\n        if userPrefs is None:\n            return True\n        return userPrefs.get('layers.async-pan-zoom.enabled', True)\n    \n    apz_disabled = pings.filter(lambda p: not apz_enabled(p))\n    apz_disabled = repartition(apz_disabled)\n    byOS = apz_disabled.map(lambda p: (p['OS'],)).countByKey()\n    byDevice = apz_disabled.map(lambda p: (p['deviceID'],)).countByKey()\n    \n    has_monitor = apz_disabled.filter(lambda p: get_monitor_count(p) > 0)\n    byResolution = has_monitor.map(lambda p: (get_monitor_res(p, 0),)).countByKey()\n    \n    return {\n        'disabled': apz_disabled.count(),\n        'byOS': byOS,\n        'byDevice': byDevice,\n        'byResolution': byResolution,\n    }\n\ndef ExportAPZStatistics():\n    pings, info = FetchAndFormat(channel = 'nightly', fraction = 1.0)\n    TimedExport(filename = 'apz-statistics.json',\n                callback = lambda: GetAPZStatistics(pings),\n                pings = (pings, info))\n    \nExportAPZStatistics()", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": null, "cell_type": "code", "source": "EndTime = datetime.datetime.now()\nTotalElapsed = (EndTime - StartTime).total_seconds()\n\nprint('Total time: {0}'.format(TotalElapsed))", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"collapsed": true, "trusted": true}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "Python 2", "name": "python2", "language": "python"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "version": "2.7.9", "name": "python", "file_extension": ".py", "pygments_lexer": "ipython2", "codemirror_mode": {"version": 2, "name": "ipython"}}}}
